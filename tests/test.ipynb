{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging in to hf and wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hamid\\OneDrive\\Documents\\ai-surveyor\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\hamid\\_netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mawsed-aq\u001b[0m (\u001b[33mawsed-aq-lut-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import wandb\n",
    "\n",
    "with open(\"../api/hf.txt\", \"r\") as f:\n",
    "    hf_token = f.read().strip()\n",
    "\n",
    "with open(\"../api/wandb.txt\", \"r\") as f:\n",
    "    wandb_token = f.read().strip()\n",
    "    \n",
    "login(token=hf_token)\n",
    "wandb.login(key=wandb_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\hamid\\OneDrive\\Documents\\ai-surveyor\\tests\\wandb\\run-20250204_165703-uori0dti</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/awsed-aq-lut-university/DeepSeek-R1-Distill-Llama-8B-ft%20for%20Surveying/runs/uori0dti?apiKey=0a628c3aa2476db82a550000885f0531d67417b9' target=\"_blank\">expert-wind-2</a></strong> to <a href='https://wandb.ai/awsed-aq-lut-university/DeepSeek-R1-Distill-Llama-8B-ft%20for%20Surveying?apiKey=0a628c3aa2476db82a550000885f0531d67417b9' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/awsed-aq-lut-university/DeepSeek-R1-Distill-Llama-8B-ft%20for%20Surveying?apiKey=0a628c3aa2476db82a550000885f0531d67417b9' target=\"_blank\">https://wandb.ai/awsed-aq-lut-university/DeepSeek-R1-Distill-Llama-8B-ft%20for%20Surveying?apiKey=0a628c3aa2476db82a550000885f0531d67417b9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/awsed-aq-lut-university/DeepSeek-R1-Distill-Llama-8B-ft%20for%20Surveying/runs/uori0dti?apiKey=0a628c3aa2476db82a550000885f0531d67417b9' target=\"_blank\">https://wandb.ai/awsed-aq-lut-university/DeepSeek-R1-Distill-Llama-8B-ft%20for%20Surveying/runs/uori0dti?apiKey=0a628c3aa2476db82a550000885f0531d67417b9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Do NOT share these links with anyone. They can be used to claim your runs."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project='DeepSeek-R1-Distill-Llama-8B-ft for Surveying', \n",
    "    job_type=\"training\", \n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries\n",
    "Also declaring the local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hamid\\OneDrive\\Documents\\ai-surveyor\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "local_directory = \"../local_model\"\n",
    "tokenizer_path = local_directory + \"/tokenizer\"\n",
    "model_path = local_directory + \"/model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model from huggingface (For the first time only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hamid\\OneDrive\\Documents\\ai-surveyor\\env\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hamid\\.cache\\huggingface\\hub\\models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model in a local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(tokenizer_path)\n",
    "model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking if CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B',\n",
    "    device_map='auto',\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "model.save_pretrained(local_directory + '/deepseek-1.5b-4bit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Configure 4-bit loading with CPU offloading\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True  # Critical for Windows\n",
    ")\n",
    "device_map = {\n",
    "    \"transformer.wte\": 0,\n",
    "    \"transformer.h.0\": 0,\n",
    "    \"transformer.h.1\": 0,\n",
    "    \"transformer.h.2\": 0,\n",
    "    \"transformer.h.3\": 0,  # First 4 layers on GPU\n",
    "    \"transformer.ln_f\": \"cpu\",  # Later layers on CPU\n",
    "    \"lm_head\": \"cpu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_directory + '/deepseek-1.5b-4bit',\n",
    "    device_map=device_map,\n",
    "    quantization_config=bnb_config,\n",
    "    offload_folder= local_directory + \"/offload\",  # Required for Windows\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained('deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B')\n",
    "\n",
    "# Create optimized pipeline\n",
    "pipe = pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    model_kwargs={\n",
    "        'use_cache': True,\n",
    "        'attn_implementation': 'sdpa'  # Flash Attention alternative\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run inference\n",
    "response = pipe(\"Explain quantum computing in simple terms:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model from the local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.85it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What type of lubricant is recommended for restoring old camera mechanisms? I'm a bit new to this, so I need to figure it out step by step. Maybe I can start by identifying the type of mechanism in the camera. Then, perhaps look into the materials used in these mechanisms. After that, I should find out the specific conditions the mechanism was in when it was taken out of the camera, like temperature, pressure, etc. Then, figure out what kind of lubricant is recommended for that scenario. Maybe I can think of some examples. For example, if the mechanism was in a high-temperature environment, maybe a silicone-based lubricant is good. Or if it was in a dry environment, maybe a silicone-based or oil-based. But I'm not sure. Maybe I should look up some examples to get a better idea. Also, maybe I should think about the specific parts of the mechanism that are prone to wear and tear, like springs, levers, gears, etc. Then, for each part, I can figure out the type of lubricant that's best. For example, springs might need a certain kind of oil or grease, while levers might need a different kind. Then, I can combine the recommendations for each part to get a comprehensive answer. Alternatively, maybe I can find a list of lubricants that are commonly used for camera mechanisms. I've heard of silicone-based lubricants being popular for things like springs and gears. Also, maybe some\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the text generation pipeline\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Hello, how are you? I have this question about a certain function. Let me try to think through it step by step.\\n\\nAlright, the question is: Let f(x) = x^2 + 2x + 3. Let g(x) be a function such that g(x) = f(x + a) + f(x - a) for some constant a. What is the value of g(0)?\\n\\nOkay, so I need to figure out g(0). Let me first write down what g(x) is. It says g(x) = f(x + a) + f(x - a). So, f is given as x squared plus 2x plus 3. So, I need to compute f(x + a) and f(x - a), add them together, and then evaluate that at x = 0.\\n\\nLet me write down f(x + a). Since f(x) is x^2 + 2x + 3, replacing x with (x + a) gives:\\n\\nf(x + a) = (x + a)^2 + 2(x + a) + 3.\\n\\nSimilarly, f(x - a) = (x - a)^2 + 2(x - a) + 3.\\n\\nSo, g(x) = f(x + a) + f(x - a). Let me compute each term separately.\\n\\nFirst, expand f(x + a):\\n\\n(x + a)^2 = x^2 + '}]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate text\n",
    "prompt = \"Hello, how are you?\"\n",
    "generated_text = generator(prompt, max_length=300, num_return_sequences=1)\n",
    "\n",
    "# Print the generated text\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
